{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network for protein contact prediction\n",
    "Model setup based on Lena, et. al - Deep architectures for protein contact map prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control = {\n",
    "    'pip': False,                 # add custom packages to environment\n",
    "    'input': 'uselocal',          # determines where to fetch input: download_raw, download_input, uselocal\n",
    "    'upload_input': False,        # upload parsed input sources to S3 for re-use\n",
    "    'experiments': 's3bucket',    # where to get experiments.json: uselocal, s3bucket\n",
    "    'exp_start': 'continue',      # if results.csv exists, determine whether to 'restart' or 'continue'\n",
    "    'run_model': True,            # determine whether to run model, otherwise only create input visulalize results\n",
    "    'results': 's3bucket',        # where to save experiments results: uselocal, s3bucket\n",
    "    's3bucket': 'protein-contact' # defines s3 bucket, can be none if not used\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if control['pip'] == False:\n",
    "    print('Skipping install of python packages')\n",
    "\n",
    "else:\n",
    "    !pip install boto3\n",
    "    !pip install requests\n",
    "    !pip install pandas\n",
    "    !pip install sklearn\n",
    "    !pip install seaborn\n",
    "    !pip install matplotlib\n",
    "    !pip install --upgrade --force-reinstall --ignore-installed tensorflow==1.13.1\n",
    "    !pip install git+git://github.com/mikepm35/biopython.git@all_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import tarfile\n",
    "import gzip\n",
    "import shutil\n",
    "import itertools as it\n",
    "import math\n",
    "import re\n",
    "import json\n",
    "\n",
    "import requests\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Bio import SeqIO, pairwise2\n",
    "from Bio.PDB.PDBParser import PDBParser\n",
    "from Bio.PDB import parse_pdb_header\n",
    "from Bio.PDB import calc_dihedral\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Astral domain data - version 2.05 <20% identity\n",
    "astral_url = 'http://scop.berkeley.edu/downloads/scopeseq-2.05/astral-scopedom-seqres-gd-sel-gs-bib-20-2.05.fa'\n",
    "astral_file = astral_url.split('/')[-1]\n",
    "\n",
    "# Astral PDB style data\n",
    "pdb_url = 'http://scop.berkeley.edu/downloads/pdbstyle/pdbstyle-sel-gs-bib-40-2.05.tgz'\n",
    "pdb_file = pdb_url.split('/')[-1]\n",
    "\n",
    "# PDB secondary structure file\n",
    "ss_url = 'https://cdn.rcsb.org/etl/kabschSander/ss.txt.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if control['input'] != 'download_raw':\n",
    "    print('Skipping download of raw data')\n",
    "\n",
    "else:\n",
    "    print('Starting download of Astral domain file')\n",
    "    urllib.request.urlretrieve(astral_url, astral_file)\n",
    "    \n",
    "    print('Starting download of Astral PDB-style data (~745MB)')\n",
    "    urllib.request.urlretrieve(pdb_url, pdb_file) # takes a while, 745MB\n",
    "    tarfile.open(pdb_file, 'r').extractall('./')\n",
    "    \n",
    "    print('Starting download of PDB secondary structure file')\n",
    "    urllib.request.urlretrieve(ss_url, ss_file)\n",
    "    with gzip.open(ss_file,\"rb\") as f_in:\n",
    "        with open(ss_file.replace('.gz',''),\"wb\") as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to parse raw data\n",
    "According to the paper, the following filters are applied on the data:\n",
    "- Length < 50 residues\n",
    "- Filter out multiple 3D structures\n",
    "- Filter out non-contiguous domains (including those with missing backbone atoms)\n",
    "- Select one representative domain (shortest one) per SCOP family\n",
    "- Ignore robustness coils, short strands (3 residues), and short helices (6 residues)\n",
    "\n",
    "DSSP classifications use the following code:\n",
    "```\n",
    "Code\tSecondary structure\n",
    "H\tα-helix\n",
    "B\tIsolated β-bridge residue\n",
    "E\tStrand\n",
    "G\t3-10 helix\n",
    "I\tΠ-helix\n",
    "T\tTurn\n",
    "S\tBend\n",
    "-\tOther\n",
    "```\n",
    "An 'X' in the sequence indicates a residue of unknown identity.\n",
    "A blank in the ss.txt file stands for a loop or other irregular structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _astral_pdb_parser(astral_id, root_dir='./pdbstyle-2.05/'):\n",
    "    \"\"\"\n",
    "    For a given astral domain id, finds and parses the PDB file.\n",
    "    Returns a dictionary with pdb_id and SCOP family.\n",
    "    Returns None if PDB file not found or doesn't meet length or gap requirements.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # header = parse_pdb_header('./pdbstyle-2.05/'+astral_id[2:4]+'/'+astral_id[2:4]+'.ent')\n",
    "        parser = PDBParser(PERMISSIVE=1)\n",
    "        structure = parser.get_structure(\n",
    "            astral_id[2:4], \n",
    "            root_dir+astral_id[2:4]+'/'+astral_id+'.ent'\n",
    "        )\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        return None\n",
    "    \n",
    "    valid = True\n",
    "        \n",
    "    # get_list() returns number of 3D models in file\n",
    "    if len(structure.get_list()) > 1:\n",
    "        valid = False\n",
    "\n",
    "    # check atom gaps\n",
    "    residue_start = None\n",
    "    residue_cnt = 0\n",
    "    for residue in structure.get_residues():\n",
    "        residue_cnt += 1\n",
    "        residue_end = residue.id[1]\n",
    "        if not residue_start:\n",
    "            residue_start = residue_end\n",
    "\n",
    "    if (residue_end - residue_start + 1) != residue_cnt:\n",
    "        valid = False  \n",
    "\n",
    "    if not valid:\n",
    "        return None\n",
    "    \n",
    "    else:\n",
    "        return {\n",
    "            'pdb_id': structure.header['astral']['Source-PDB'], \n",
    "            'family': structure.header['astral']['SCOPe-sccs']\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_astral_domains(filename=astral_file):\n",
    "    \"\"\"\n",
    "    Uses Biopython module to parse astral fasta files.\n",
    "    Returns dataframe filters for shortest domain length per family.\n",
    "    \"\"\"\n",
    "    \n",
    "    print('Starting _get_astral_domains()...')\n",
    "    \n",
    "    astral_records = []\n",
    "    for seq_record in SeqIO.parse(filename, \"fasta\"):\n",
    "        pdb_data = _astral_pdb_parser(seq_record.id)\n",
    "\n",
    "        if len(seq_record) >= 50 and pdb_data:\n",
    "            astral_records.append({\n",
    "                'astral_id': seq_record.id, \n",
    "                'ss_id': seq_record.id[1:-1],\n",
    "                'astral_sequence': str(seq_record.seq),\n",
    "                'astral_sequence_len': len(seq_record.seq),\n",
    "                'pdb_id': pdb_data['pdb_id'],\n",
    "                'family': pdb_data['family']\n",
    "            })\n",
    "\n",
    "    df_domains = pd.DataFrame(astral_records)\n",
    " \n",
    "    # filter to shortert domain length per family\n",
    "    idx_family_max = df_domains.groupby(['family'])['astral_sequence_len'].transform(max) == df_domains['astral_sequence_len']\n",
    "    df_domains = df_domains[idx_family_max]\n",
    "    \n",
    "    # filter out domains with multiple families\n",
    "    idx_family_count = df_domains.groupby(['ss_id'])['family'].transform('size') == 1\n",
    "    df_domains = df_domains[idx_family_count]\n",
    "    \n",
    "    return df_domains\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_secondary_structures(filename='ss.txt'):\n",
    "    \"\"\"\n",
    "    Parses PDB secondary structure file according to dssp classification and returns two dataframes:\n",
    "        df_ss: secondary structure sequences with pdb id as key\n",
    "        df_sse: individual elements for a given pdb id with pdb_id and element index as key\n",
    "        \n",
    "    Filter out any elements that have multiple structures\n",
    "    \"\"\"\n",
    "    \n",
    "    print('Starting _get_secondary_structures()...')\n",
    "\n",
    "    ss_records = []\n",
    "    sse_records = []\n",
    "    ss_ids = set()\n",
    "    ss_id_dups = set()\n",
    "    for ss_record in SeqIO.parse('ss.txt', \"pdb-ss\"):\n",
    "        title = ss_record.id.split(':')\n",
    "\n",
    "        if title[2].lower() != 'secstr':\n",
    "            continue    \n",
    "\n",
    "        pdb_id = title[0].lower()\n",
    "        ss_id = pdb_id + title[1].lower()\n",
    "        ss_record_seq = str(ss_record.seq).replace('\\n','').replace(' ','_')\n",
    "                \n",
    "        if ss_id in ss_ids:\n",
    "            ss_id_dups.add(ss_id)\n",
    "            continue\n",
    "        else:\n",
    "            ss_ids.add(ss_id)\n",
    "\n",
    "        ss_records.append({\n",
    "            'pdb_id': pdb_id,\n",
    "            'ss_id': ss_id,\n",
    "            'ss_sequence': ss_record_seq,\n",
    "            'ss_sequence_len': len(ss_record_seq)\n",
    "        })\n",
    "\n",
    "        # get element records\n",
    "        ind_end = -1\n",
    "        ind_element = -1\n",
    "\n",
    "        for dssp, group in it.groupby(ss_record_seq):\n",
    "            length = len(list(group))\n",
    "            ind_element = ind_element + 1\n",
    "            ind_start = ind_end + 1\n",
    "            ind_end = ind_start + length - 1\n",
    "\n",
    "            ignore = False\n",
    "            if (length <= 3) or (length <=6 and dssp in ('H','G','I')) or (dssp in ('-','_','X')):\n",
    "                ignore = True\n",
    "\n",
    "            # ~80% are ignored, only store non-ignored to conserve memory\n",
    "            if not ignore:\n",
    "                sse_records.append({\n",
    "                    'sse_key': ss_id+'.'+str(ind_element),\n",
    "                    'pdb_id': pdb_id,\n",
    "                    'ss_id': ss_id,\n",
    "                    'ind_element': ind_element,\n",
    "                    'ind_start': ind_start,\n",
    "                    'ind_end': ind_end,\n",
    "                    'length': length,\n",
    "                    'dssp': dssp,\n",
    "                    'ignore': ignore\n",
    "                })\n",
    "\n",
    "\n",
    "    df_ss = pd.DataFrame(ss_records)\n",
    "    df_sse = pd.DataFrame(sse_records)\n",
    "    \n",
    "    # remove duplicates\n",
    "    df_ss = df_ss[~df_ss['ss_id'].isin(ss_id_dups)]\n",
    "    df_sse = df_sse[~df_sse['ss_id'].isin(ss_id_dups)]\n",
    "    \n",
    "    return df_ss, df_sse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_domain_sse_dfs():\n",
    "    \"\"\"\n",
    "    Retrieves domain and secondary structure dataframes.\n",
    "    \"\"\"\n",
    "    \n",
    "    df_domains = _get_astral_domains()\n",
    "    df_ss, df_sse = _get_secondary_structures()\n",
    "    \n",
    "    # Merge domain records with secondary structure and exclude where lengths do no match\n",
    "    print('Starting merge of domain records with secondary structure...')\n",
    "    df_domains = df_domains.merge(df_ss, on='ss_id', how='inner', suffixes=('','_ss'))\n",
    "    df_domains = df_domains[df_domains['astral_sequence_len']==df_domains['ss_sequence_len']]\n",
    "    \n",
    "    # Filter sse dataframe to only those domains remaining\n",
    "    valid_ss_ids = set(df_domains['ss_id'].tolist())\n",
    "    df_sse = df_sse[df_sse['ss_id'].isin(valid_ss_ids)]\n",
    "    \n",
    "    return df_domains, df_sse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to create paired data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sse_pairs(df_sse):\n",
    "    \"\"\"\n",
    "    Creates dataframe of valid sse pairs by pdb id\n",
    "    \"\"\"\n",
    "    \n",
    "    sse_pairs = []\n",
    "    \n",
    "    # Find pdb ids that have more than one non-ignored secondary structure elements\n",
    "    ss_ids = list(set(df_sse[df_sse['ignore']==False].groupby('ss_id').filter(lambda x: x['ss_id'].count() > 1)['ss_id'].tolist()))\n",
    "    \n",
    "    # For each pdb id create unique pairs\n",
    "    for ss_id in ss_ids:\n",
    "        sse_keys = df_sse[(df_sse['ss_id']==ss_id)&(df_sse['ignore']==False)]['sse_key'].tolist()\n",
    "        \n",
    "        for idx_outer, sse_key_1 in enumerate(sse_keys):\n",
    "            for idx_inner, sse_key_2 in enumerate(sse_keys):\n",
    "                if idx_inner > idx_outer:\n",
    "                    sse_pairs.append({\n",
    "                        'ss_id': ss_id,\n",
    "                        'sse_key_1': sse_key_1,\n",
    "                        'sse_key_2': sse_key_2,\n",
    "                        'sse_pair_key': sse_key_1+'&'+sse_key_2\n",
    "                    })\n",
    "    \n",
    "    df_sse_pairs = pd.DataFrame(sse_pairs)\n",
    "    \n",
    "    return df_sse_pairs\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sse_neighbors(df_sse, df_sse_pairs, max_radius=7):\n",
    "    \"\"\"\n",
    "    Create map of neighboring sse_pairs in receptive radius attribute\n",
    "    \"\"\"\n",
    "        \n",
    "    # Map sse data onto pairs\n",
    "    df_sse_pairs_m = df_sse_pairs.merge(df_sse, left_on='sse_key_1', right_on='sse_key', suffixes=('', '_1'))\n",
    "    df_sse_pairs_m = df_sse_pairs_m.merge(df_sse, left_on='sse_key_2', right_on='sse_key', suffixes=('', '_2'))\n",
    "    \n",
    "    # Self join sse data and exclude exact same pairs\n",
    "    df_sse_n = df_sse_pairs_m.merge(df_sse_pairs_m, on='ss_id', how='left', suffixes=('', '_n'))\n",
    "    df_sse_n = df_sse_n[df_sse_n['sse_pair_key']!=df_sse_n['sse_pair_key_n']]\n",
    "    \n",
    "    # Determine receptive radiuses\n",
    "    df_sse_n['radius_1_1'] = (df_sse_n['ind_element']-df_sse_n['ind_element_n']).abs()\n",
    "    df_sse_n['radius_1_2'] = (df_sse_n['ind_element']-df_sse_n['ind_element_2_n']).abs()\n",
    "    df_sse_n['radius_2_1'] = (df_sse_n['ind_element_2']-df_sse_n['ind_element_n']).abs()\n",
    "    df_sse_n['radius_2_2'] = (df_sse_n['ind_element_2']-df_sse_n['ind_element_2_n']).abs()\n",
    "    \n",
    "    df_sse_n['max_radius'] = df_sse_n[['radius_1_1','radius_1_2','radius_2_1','radius_2_2']].max(axis=1)\n",
    "    \n",
    "    # Filter by max radius\n",
    "    df_sse_n = df_sse_n[df_sse_n['max_radius']<=max_radius]\n",
    "    \n",
    "    # Return subset of columns\n",
    "    df_sse_n = df_sse_n[['ss_id', 'sse_pair_key', 'sse_pair_key_n', 'max_radius']]\n",
    "    \n",
    "    return df_sse_n    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to create feature and label vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_sse_feature_vector(row, row_m1, row_p1, sse_keys):\n",
    "    \"\"\"\n",
    "    Creates a feature vector of 109 elements for a given secondary structure element (sse)\n",
    "    \n",
    "    featurevector = [\n",
    "        ['20: aa distribution for s-1'],\n",
    "        ['20: aa distribution for s'],\n",
    "        ['20: aa distribution for s+1'],\n",
    "        ['3: residue len s-1, s, s+1'],\n",
    "        ['2: residue len between s-1/s and s/s+1; zero for adjacent elements']\n",
    "        ['4: binary flags for first, second, second-to-last, last element in sequence'],\n",
    "        ['20: aa distribution for odd residues in s'],\n",
    "        ['20: aa distribution for even residues in s']\n",
    "    ]\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Amino acid order key\n",
    "    aa_order = ('A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y')\n",
    "\n",
    "    # Get element sequence flag (assumes only looking at non-ignored sequences)\n",
    "    if row['sse_key'] == sse_keys[0]:\n",
    "        position_flag = [1,0,0,0]\n",
    "    elif row['sse_key'] == sse_keys[-1]:\n",
    "        position_flag = [0,0,0,1]\n",
    "    elif row['sse_key'] == sse_keys[1]:\n",
    "        position_flag = [0,1,0,0]\n",
    "    elif row['sse_key'] == sse_keys[-2]:\n",
    "        position_flag = [0,0,1,0]\n",
    "    else:\n",
    "        position_flag = [0,0,0,0]\n",
    "    \n",
    "    # Amino acid distributions for Sn\n",
    "    aa_s = row['astral_sequence'][row['ind_start']:row['ind_end']]\n",
    "    aa_odd_s = aa_s[1::2]\n",
    "    aa_even_s = aa_s[::2]\n",
    "    \n",
    "    aadistrodict_s = ProteinAnalysis(aa_s).get_amino_acids_percent()\n",
    "    aadistro_s = []\n",
    "    for aa in aa_order:\n",
    "        aadistro_s.append(aadistrodict_s[aa])\n",
    " \n",
    "    aadistrodict_s = ProteinAnalysis(aa_odd_s).get_amino_acids_percent()\n",
    "    aadistro_odd_s = []\n",
    "    for aa in aa_order:\n",
    "        aadistro_odd_s.append(aadistrodict_s[aa])\n",
    "        \n",
    "    aadistrodict_s = ProteinAnalysis(aa_even_s).get_amino_acids_percent()\n",
    "    aadistro_even_s = []\n",
    "    for aa in aa_order:\n",
    "        aadistro_even_s.append(aadistrodict_s[aa])\n",
    "        \n",
    "    # Amino acid distributions for Sn-1, use blank space to get empty distro\n",
    "    if position_flag[0] == 1 or row_m1 is None:\n",
    "        aa_s_m1 = \" \"\n",
    "    else:\n",
    "        aa_s_m1 = row_m1['astral_sequence'][row_m1['ind_start']:row_m1['ind_end']]\n",
    "  \n",
    "    aadistrodict_s_m1 = ProteinAnalysis(aa_s_m1).get_amino_acids_percent()\n",
    "    aadistro_s_m1 = []\n",
    "    for aa in aa_order:\n",
    "        aadistro_s_m1.append(aadistrodict_s_m1[aa])\n",
    "\n",
    "    # Amino acid distributions for Sn+1, use blank space to get empty distro\n",
    "    if position_flag[3] == 1 or row_p1 is None:\n",
    "        aa_s_p1 = \" \"\n",
    "    else:\n",
    "        aa_s_p1 = row_p1['astral_sequence'][row_p1['ind_start']:row_p1['ind_end']]\n",
    "  \n",
    "    aadistrodict_s_p1 = ProteinAnalysis(aa_s_p1).get_amino_acids_percent()\n",
    "    aadistro_s_p1 = []\n",
    "    for aa in aa_order:\n",
    "        aadistro_s_p1.append(aadistrodict_s_p1[aa])\n",
    "        \n",
    "    # Residue lengths\n",
    "    lens = [\n",
    "        len(aa_s_m1) if aa_s_m1 != \" \" else 0, \n",
    "        len(aa_s), \n",
    "        len(aa_s_p1) if aa_s_p1 != \" \" else 0\n",
    "    ]\n",
    "    \n",
    "    # Residue gaps\n",
    "    if position_flag[0] == 1 or row_m1 is None or (row['ind_element']-row_m1['ind_element']) == 0:\n",
    "        gap_s_m1 = 0\n",
    "    else:\n",
    "        gap_s_m1 = row['ind_start'] - row_m1['ind_end'] + 1\n",
    "        \n",
    "    if position_flag[3] == 1 or row_p1 is None or (row_p1['ind_element']-row['ind_element']) == 0:\n",
    "        gap_s_p1 = 0\n",
    "    else:\n",
    "        gap_s_p1 = row_p1['ind_start'] - row['ind_end'] + 1  \n",
    "\n",
    "    gaps = [gap_s_m1, gap_s_p1]\n",
    "    \n",
    "    # Return combined feature vector\n",
    "    return aadistro_s_m1 + aadistro_s + aadistro_s_p1 + lens + gaps + position_flag + aadistro_odd_s + aadistro_even_s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_sse_pair_label_vector(row, root_dir='./pdbstyle-2.05/'):\n",
    "    \"\"\"\n",
    "    Retrieve the label vector which determines the contact type.\n",
    "    Requires reading the PDB files to calculate geometry.\n",
    "    \n",
    "    label_vector = ['3: probability of parallel contact, anti-parallel contact or no-contact']\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Get PDB structure for each element  \n",
    "    parser = PDBParser(PERMISSIVE=1)\n",
    "    structure = parser.get_structure(\n",
    "        row['astral_id'][2:4], \n",
    "        root_dir+row['astral_id'][2:4]+'/'+row['astral_id']+'.ent'\n",
    "    )\n",
    "    \n",
    "    # Get residues and assign to ss elements, fill in gaps for missing residues\n",
    "    residues1 = []\n",
    "    residues2 = []\n",
    "    ind = 0\n",
    "    res_ind_start = -1000\n",
    "    for residue in structure.get_residues():\n",
    "        if res_ind_start == -1000:\n",
    "            res_ind_start = residue.id[1]\n",
    "        elif residue.id[1] != ind + res_ind_start:\n",
    "            gap = residue.id[1] - ind - res_ind_start\n",
    "            #print('Missing residues, adv ind by %s (%s, %s, %s, %s)' % (gap, rec['fasta'].name, residue.id[1], ind, res_ind_start))\n",
    "            ind += gap\n",
    "        \n",
    "        if row['ind_start'] <= ind <= row['ind_end']:\n",
    "            residues1.append(residue)\n",
    "        elif row['ind_start_b'] <= ind <= row['ind_end_b']:\n",
    "            residues2.append(residue)            \n",
    "        \n",
    "        ind += 1\n",
    "        \n",
    "    if len(residues1)<=1 or len(residues2)<=1:\n",
    "        return None\n",
    "             \n",
    "    # Calculate angle between two ss elements\n",
    "    angle = abs(calc_dihedral(\n",
    "        residues1[1]['CA'].get_vector(), \n",
    "        residues1[-1]['CA'].get_vector(), \n",
    "        residues2[1]['CA'].get_vector(), \n",
    "        residues2[-1]['CA'].get_vector()\n",
    "    ) * 180/math.pi) \n",
    "\n",
    "    # Iterate over residuce pairings and calculate distance\n",
    "    distances = []\n",
    "    for residue1 in residues1:\n",
    "        for residue2 in residues2:\n",
    "            distances.append(abs(residue1['CA'] - residue2['CA']))\n",
    "            \n",
    "    min_distance = min(distances)\n",
    "            \n",
    "    # Determine contact type\n",
    "    if min_distance >= 8:\n",
    "        label_vector = [0,0,1]\n",
    "    elif angle < 90:\n",
    "        label_vector = [0,1,0]\n",
    "    elif angle >= 90:\n",
    "        label_vector = [1,0,0]\n",
    "    else:\n",
    "        raise Exception('Unknown contact type')\n",
    "\n",
    "    return label_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sse_features(df_domains, df_sse):\n",
    "    \"\"\"\n",
    "    Retrieve feature vectors for each sse. Utilizes data from neighboring sse's as well.\n",
    "    \"\"\"\n",
    "    \n",
    "    print('Starting get_sse_features()...')\n",
    "    \n",
    "    sse_features = []\n",
    "    \n",
    "    # group sse indicies by ss_id\n",
    "    df_sse_grouped = df_sse[['ss_id','sse_key']].groupby('ss_id', as_index=False).aggregate(lambda x: tuple(x))\n",
    "    df_sse_m = df_sse.merge(df_sse_grouped, on='ss_id', suffixes=('','_m'))\n",
    "    \n",
    "    # merge with domain data\n",
    "    df_sse_m = df_sse_m.merge(df_domains, on='ss_id', suffixes=('','_d'))\n",
    "    \n",
    "    # find neighboring elements and pass to feature vector calculation\n",
    "    for index, row in df_sse_m.iterrows():\n",
    "        sse_keys = row['sse_key_m']\n",
    "        \n",
    "        sse_key_idx = sse_keys.index(row['sse_key'])\n",
    "        \n",
    "        row_m1 = None\n",
    "        if sse_key_idx > 0:\n",
    "            row_m1 = df_sse_m[df_sse_m['sse_key']==sse_keys[sse_key_idx-1]].iloc[0]\n",
    "            \n",
    "        row_p1 = None\n",
    "        if sse_key_idx < len(sse_keys)-1:\n",
    "            row_p1 = df_sse_m[df_sse_m['sse_key']==sse_keys[sse_key_idx+1]].iloc[0]\n",
    "            \n",
    "        feature_vector = _get_sse_feature_vector(row, row_m1, row_p1, sse_keys)\n",
    "        \n",
    "        feature_vector = [row['sse_key']] + feature_vector\n",
    "        \n",
    "        sse_features.append(feature_vector)\n",
    "        \n",
    "    \n",
    "    df_sse_features = pd.DataFrame(sse_features)\n",
    "    df_sse_features = df_sse_features.rename(index=str, columns={0: 'sse_key'})\n",
    "    \n",
    "    return df_sse_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sse_pair_labels(df_domains, df_sse, df_sse_pairs):\n",
    "    \"\"\"\n",
    "    Retrieve contact type based on residue geometry for each pair.\n",
    "    \"\"\"\n",
    "    \n",
    "    print('Starting get_sse_pair_labels()...')\n",
    "    \n",
    "    sse_pair_labels = []\n",
    "    \n",
    "    # Join SSE and domain data into pairs\n",
    "    df_sse_pairs_m = df_sse_pairs.merge(df_domains, on='ss_id', suffixes=('', '_d'))\n",
    "    df_sse_pairs_m = df_sse_pairs_m.merge(df_sse, left_on='sse_key_1', right_on='sse_key', suffixes=('','_a'))\n",
    "    df_sse_pairs_m = df_sse_pairs_m.merge(df_sse, left_on='sse_key_2', right_on='sse_key', suffixes=('','_b'))\n",
    "    \n",
    "    # Retrieve label vector\n",
    "    for index, row in df_sse_pairs_m.iterrows():\n",
    "        label_vector = _get_sse_pair_label_vector(row)\n",
    "        \n",
    "        if label_vector is None:\n",
    "            continue\n",
    "        \n",
    "        label_vector = [row['sse_pair_key'], row['sse_key_1'], row['sse_key_2']] + label_vector\n",
    "        \n",
    "        sse_pair_labels.append(label_vector)\n",
    "        \n",
    "    \n",
    "    df_pair_labels = pd.DataFrame(sse_pair_labels)\n",
    "    df_pair_labels = df_pair_labels.rename(index=str, columns={0: 'sse_pair_key', 1: 'sse_key_1', 2: 'sse_key_2'})\n",
    "    \n",
    "    return df_pair_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3client = boto3.client('s3')\n",
    "csvs = ['sse_features.csv', 'pair_labels.csv', 'sse_pairs.csv', 'sse_neighbors.csv']\n",
    "\n",
    "\n",
    "if control['input'] == 'download_raw':\n",
    "    print('Parsing raw data to generate inputs...')\n",
    "    \n",
    "    # get parsed raw data with filters applied\n",
    "    df_domains, df_sse = get_domain_sse_dfs()\n",
    "    df_sse_pairs = get_sse_pairs(df_sse)\n",
    "    df_sse_neighbors = get_sse_neighbors(df_sse, df_sse_pairs)\n",
    "    \n",
    "    # get feature data by secondary structure element (sse)\n",
    "    df_sse_features = get_sse_features(df_domains, df_sse)\n",
    "\n",
    "    # get output/label data by pair\n",
    "    df_pair_labels = get_sse_pair_labels(df_domains, df_sse, df_sse_pairs)\n",
    "\n",
    "elif control['input'] == 'download_input':\n",
    "    print('Downloading input data from S3...')\n",
    "    \n",
    "    # download from s3\n",
    "    for csv in csvs:\n",
    "        s3client.download_file(control['s3bucket'], csv, './'+csv)\n",
    "\n",
    "        \n",
    "if control['input'] in ('uselocal', 'download_input'):\n",
    "    print('Reading local input csv files...')\n",
    "    # read csv files\n",
    "    df_sse_features = pd.read_csv('./sse_features.csv')\n",
    "    df_pair_labels = pd.read_csv('./pair_labels.csv')\n",
    "    df_sse_pairs = pd.read_csv('./sse_pairs.csv')\n",
    "    df_sse_neighbors = pd.read_csv('./sse_neighbors.csv')    \n",
    "\n",
    "\n",
    "if control['upload_input'] == True:\n",
    "    print('Uploading input data to S3...')\n",
    "\n",
    "    # create csv files\n",
    "    df_sse_features.to_csv('./sse_features.csv', header=True, index=False)\n",
    "    df_pair_labels.to_csv('./pair_labels.csv', header=True, index=False)\n",
    "    df_sse_pairs.to_csv('./sse_pairs.csv', header=True, index=False)\n",
    "    df_sse_neighbors.to_csv('./sse_neighbors.csv', header=True, index=False)\n",
    "\n",
    "    # upload to s3\n",
    "    for csv in csvs:\n",
    "        s3client.upload_file('./'+csv, control['s3bucket'], csv)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize residue length in feature data\n",
    "df_sse_features_normd = df_sse_features.copy()\n",
    "\n",
    "for c in ['61','62','63','64','65']:\n",
    "    c_max = df_sse_features_normd[[c]].max()\n",
    "    c_min = df_sse_features_normd[[c]].min()\n",
    "    \n",
    "    df_sse_features_normd[c] = df_sse_features_normd[c].apply(lambda x: (x-c_min)/(c_max-c_min))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct 2D features dataframe\n",
    "df_features_2d = df_sse_pairs.merge(df_sse_features_normd, how='inner', left_on='sse_key_1', right_on='sse_key', suffixes=('','_a'))\n",
    "df_features_2d = df_features_2d.merge(df_sse_features_normd, left_on='sse_key_2', right_on='sse_key', suffixes=('','_b'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct labels dataframe\n",
    "df_labels = df_sse_pairs.merge(df_pair_labels, how='inner', on='sse_pair_key', suffixes=('','_a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty numpy arrays\n",
    "max_pairs = (7*2+1)**2 # receptive radius of 7\n",
    "np_features = np.zeros((df_sse_pairs.shape[0],max_pairs,109*2), dtype=np.float32)\n",
    "np_labels = np.zeros((df_sse_pairs.shape[0],3), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 2D data to arrays\n",
    "features_2d_cols = [str(i) for i in range(1,110)] + [str(i)+'_b' for i in range(1,110)]\n",
    "np_features[:,0,:] = df_features_2d[features_2d_cols].values\n",
    "\n",
    "labels_cols = ['3','4','5']\n",
    "np_labels[:,:] = df_labels[labels_cols].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add neighbor data into second dimension of features\n",
    "df_features_n = df_features_2d.merge(df_sse_neighbors, on='sse_pair_key', suffixes=('','_n1'))\n",
    "df_features_n = df_features_n.merge(df_features_2d, left_on='sse_pair_key_n', right_on='sse_pair_key', suffixes=('','_n2'))\n",
    "\n",
    "features_n_cols = [str(i)+'_n2' for i in range(1,110)] + [str(i)+'_b_n2' for i in range(1,110)]\n",
    "                                                      \n",
    "for index, row in df_features_2d.reset_index().iterrows(): # index matches np array\n",
    "    df_n = df_features_n[(df_features_n['sse_pair_key']==row['sse_pair_key']) & (df_features_n['max_radius']<=7)].sort_values(by='max_radius', ascending=True)\n",
    "    \n",
    "    if df_n.shape[0] > 0:\n",
    "        np_features[index,1:df_n.shape[0]+1,:] = df_n[features_n_cols].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of pairs for fitting: %s' % np_features.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(np.array([np.where(label==1)[0][0] for label in np_labels]), return_counts=True)\n",
    "class_counts = dict(zip(unique, counts))\n",
    "\n",
    "label_dist = {\n",
    "    'parallel': round(class_counts[0]/np_labels.shape[0],4), \n",
    "    'anti-parallel': round(class_counts[1]/np_labels.shape[0],4),\n",
    "    'no-contact': round(class_counts[2]/np_labels.shape[0],4)\n",
    "}\n",
    "\n",
    "plt.bar(np.arange(3), [label_dist[k] for k in label_dist], align='center', alpha=0.5)\n",
    "plt.xticks(np.arange(3), label_dist.keys())\n",
    "plt.ylabel('Fractional representation')\n",
    "plt.title('Contact type distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radii = list(range(0,8))\n",
    "map_inds = [(i*2+1)**2 for i in radii]\n",
    "\n",
    "plt.plot(radii, map_inds)\n",
    "plt.title(\"Count of additional feature rows for given neighborhood radius\")\n",
    "plt.ylabel('Feature rows')\n",
    "plt.xlabel('Neighborhood radius')\n",
    "plt.show()\n",
    "\n",
    "map_sparsity = []\n",
    "for map_ind in map_inds:\n",
    "    map_sparsity.append((np_features[:,0:map_ind,:]==0.0).sum() / (np_features.shape[0]*np_features.shape[2]*map_ind))\n",
    "\n",
    "plt.plot(radii, map_sparsity)\n",
    "plt.title(\"Overall feature sparsity by neighborhood map radius\")\n",
    "plt.ylabel('Percent of values equaling zero')\n",
    "plt.xlabel('Neighborhood radius')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_sse_features[[str(i) for i in range(21,41)]].values.flatten(), bins='auto', density=True)\n",
    "plt.title(\"Histogram of amino acid distribution\")\n",
    "plt.ylabel('Count of values')\n",
    "plt.xlabel('Amino acid fractional occurrence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_sse_features[['62']].values.flatten(), bins='auto', density=True)\n",
    "plt.title(\"Histogram of non-normalized residue length\")\n",
    "plt.ylabel('Count of values')\n",
    "plt.xlabel('Residue length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_sse_features[['64','65']].values.flatten(), bins='auto', density=True)\n",
    "plt.title(\"Histogram of non-normalized residue spacing\")\n",
    "plt.ylabel('Count of values')\n",
    "plt.xlabel('Residue spacing')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Generates protein-contact specific metrics by epoch on both training and validation datasets\n",
    "    using sklearn classification report.    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, train_data, validation_data):\n",
    "        super().__init__()\n",
    "        self.validation_data = validation_data\n",
    "        self.train_data = train_data\n",
    "        self.epochs = 0\n",
    "        self.metrics = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        self.epochs += 1\n",
    "\n",
    "        x_train = self.train_data[0]\n",
    "        y_train_true = self.train_data[1]\n",
    "        y_train_pred = self.model.predict(x_train)\n",
    "        \n",
    "        x_test = self.validation_data[0]\n",
    "        y_test_true = self.validation_data[1]\n",
    "        y_test_pred = self.model.predict(x_test)\n",
    "                \n",
    "        epoch_results = []\n",
    "        \n",
    "        classes = {\n",
    "            '0': 'parallel', '1': 'anti-parallel', '2': 'no-contact', \n",
    "            'macro avg': 'all_macro', 'weighted avg': 'all_weighted'\n",
    "        }\n",
    "        \n",
    "        report = {\n",
    "            'train': classification_report(np.argmax(y_train_true, axis=-1), np.argmax(y_train_pred, axis=-1), output_dict=True),\n",
    "            'test': classification_report(np.argmax(y_test_true, axis=-1), np.argmax(y_test_pred, axis=-1), output_dict=True)\n",
    "        }\n",
    "        \n",
    "        for i in classes.keys():\n",
    "            for d in ('train', 'test'):\n",
    "                if 'all_' in classes[i]:\n",
    "                    accuracy = report[d]['accuracy']\n",
    "                else:\n",
    "                    accuracy = 0\n",
    "            \n",
    "                epoch_results.append({\n",
    "                    'epoch': self.epochs,\n",
    "                    'dataset': d,\n",
    "                    'class': classes[i],\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': report[d][i]['precision'],\n",
    "                    'recall': report[d][i]['recall'],\n",
    "                    'f1_score': report[d][i]['f1-score'],\n",
    "                    'support': report[d][i]['support']\n",
    "                })          \n",
    "        \n",
    "        self.metrics += epoch_results\n",
    "                    \n",
    "        return epoch_results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(layers_def, input_shape, learning_rate):\n",
    "    \"\"\"\n",
    "    layers is a list of dictonaries describing how to build the sequential model:\n",
    "    {\n",
    "        'layer_type': <e.g. 'LSTM'>,\n",
    "        'units': <e.g. 8>,\n",
    "        'activation': <optional, e.g 'tanh'>,\n",
    "        'return_sequences': <optional, e.g. True>,\n",
    "        'input_shape': <optional, e.g. (2, 219)\n",
    "    }\n",
    "    \n",
    "    Retruns compiled model\n",
    "    \"\"\"\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    rnn_shape = input_shape\n",
    "    \n",
    "    for layer in layers_def:\n",
    "        if layer['layer_type'] == 'LSTM':\n",
    "            model.add(\n",
    "                layers.LSTM(\n",
    "                    layer['units'], \n",
    "                    return_sequences=layer['return_sequences'], \n",
    "                    activation=layer['activation'], \n",
    "                    input_shape=rnn_shape\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            rnn_shape = (layer['units'], rnn_shape[1])\n",
    "        \n",
    "        elif layer['layer_type'] == 'GRU':\n",
    "            model.add(\n",
    "                layers.GRU(\n",
    "                    layer['units'], \n",
    "                    return_sequences=layer['return_sequences'], \n",
    "                    activation=layer['activation'], \n",
    "                    input_shape=rnn_shape\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            rnn_shape = (layer['units'], rnn_shape[1])\n",
    "        \n",
    "        elif layer['layer_type'] == 'Dense':\n",
    "            model.add(\n",
    "                layers.Dense(\n",
    "                    layer['units'], \n",
    "                    activation=layer['activation']\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        elif layer['layer_type'] == 'Dropout':\n",
    "            model.add(\n",
    "                layers.Dropout(\n",
    "                    layer['units']\n",
    "                )\n",
    "            )            \n",
    "\n",
    "    model.compile(optimizer=tf.train.AdamOptimizer(learning_rate),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=[\n",
    "                      'accuracy', # true positives / total, equals Q3\n",
    "                      tf.keras.metrics.Precision(), # TP/(TP+FP) = PPV (positive predictive value)\n",
    "                      tf.keras.metrics.Recall(), # TP/(TP+FN) = TPR (true positive rate)\n",
    "                  ]\n",
    "    )\n",
    "        \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_and_split_inds(np_features, np_labels, split=0.9, seed=32):\n",
    "    \"\"\"\n",
    "    Takes a feature and label arrays and creates shuffled data split into train and validation datasets.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed) # predicatable random distribution\n",
    "    inds = list(range(np_features.shape[0]))\n",
    "    np.random.shuffle(inds)\n",
    "    \n",
    "    train_len = int(len(inds)*split)\n",
    "\n",
    "    np_features_train = np_features[inds[:train_len]][:][:]\n",
    "    np_labels_train = np_labels[inds[:train_len]][:][:]\n",
    "\n",
    "    np_features_val = np_features[inds[train_len:]][:][:]\n",
    "    np_labels_val = np_labels[inds[train_len:]][:][:]    \n",
    "    \n",
    "    return np_features_train, np_labels_train, np_features_val, np_labels_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalize_history(history):\n",
    "    \"\"\"\n",
    "    Removes extraneous identifiers on keras history data\n",
    "    \"\"\"\n",
    "    \n",
    "    history_gen = dict()\n",
    "    \n",
    "    for key in history:\n",
    "        key_gen = re.sub(r'_[0-9]+', '', key)\n",
    "        history_gen[key_gen] = history[key]    \n",
    "    \n",
    "    return history_gen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiments_file(location, s3bucket=None):\n",
    "    \"\"\"\n",
    "    Converts experiments.json to dictionary per location (uselocal, s3bucket)\n",
    "    \"\"\"\n",
    "    \n",
    "    if location == 's3bucket':\n",
    "        print('Downloading experiments.json from s3bucket %s' % s3bucket)\n",
    "        boto3.client('s3').download_file(s3bucket, 'experiments.json', './experiments.json')\n",
    "        \n",
    "    # read json file\n",
    "    with open('experiments.json') as json_file:  \n",
    "        experiments = json.load(json_file)\n",
    "    \n",
    "    return experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(location, s3bucket=None):\n",
    "    \"\"\"\n",
    "    Load results file as defined by location ('uselocal', 's3bucket').\n",
    "    Returns dataframe or none if file does not exist.\n",
    "    \"\"\"\n",
    "    \n",
    "    if location == 's3bucket':\n",
    "        print('Downloading results.csv from s3bucket %s' % s3bucket)\n",
    "        try:\n",
    "            boto3.client('s3').download_file(s3bucket, 'results.csv', './results.csv')\n",
    "        except botocore.exceptions.ClientError as e:\n",
    "            if e.response['Error']['Code'] != '404':\n",
    "                raise\n",
    "            else:\n",
    "                print('results.csv not found on s3bucket')\n",
    "                return None\n",
    "        \n",
    "    # read csv file\n",
    "    try:\n",
    "        df_results = pd.read_csv('./results.csv')\n",
    "    except FileNotFoundError:\n",
    "        df_results = None\n",
    "    \n",
    "    return df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_results(df, experiment):\n",
    "    \"\"\"\n",
    "    Converts model history and MetricsCallback to a single row result.\n",
    "    \"\"\"\n",
    "    \n",
    "    last_epoch = len(experiment['history']['acc'])\n",
    "    \n",
    "    train_loss = experiment['history']['loss'][last_epoch-1]\n",
    "    val_loss = experiment['history']['val_loss'][last_epoch-1]\n",
    "    \n",
    "    metrics = [i for i in experiment['metrics_callback'] if i['epoch']==last_epoch]\n",
    "    \n",
    "    for metric in metrics:\n",
    "        metric['id'] = experiment['id']\n",
    "        \n",
    "        if metric['class'] in ('all_macro','all_weighted'):\n",
    "            if metric['dataset'] == 'test':\n",
    "                metric['loss'] = val_loss\n",
    "            elif metric['dataset'] == 'train':\n",
    "                metric['loss'] = train_loss\n",
    "        else:\n",
    "            metric['loss'] = 0.0\n",
    "    \n",
    "\n",
    "    if df is None:\n",
    "        df = pd.DataFrame(metrics)\n",
    "    else:\n",
    "        df_new = pd.DataFrame(metrics)\n",
    "        df = df.append(df_new, ignore_index=True)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_summary_results(df, location, s3bucket=None):\n",
    "    \"\"\"\n",
    "    Save results to local file and optionally upload to S3.\n",
    "    \"\"\"\n",
    "    \n",
    "    df.to_csv('./results.csv', header=True, index=False)\n",
    "\n",
    "    if location == 's3bucket':\n",
    "        boto3.client('s3').upload_file('./results.csv', s3bucket, 'results.csv') \n",
    "    \n",
    "    return 'done'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(np_features, np_labels, experiment):\n",
    "    \"\"\"\n",
    "    Takes in features and labels as numpy arrays and runs the model according\n",
    "    to parameters to experiment dictionary.\n",
    "    \n",
    "    Returns model_fit and metrics_callback\n",
    "    \"\"\"\n",
    "    # shuffle and split data\n",
    "    np_features_train, np_labels_train, np_features_val, np_labels_val = shuffle_and_split_inds(np_features, np_labels)\n",
    "    \n",
    "    # modify feature data depending on neighbor radius\n",
    "    if experiment['radius'] == 0:\n",
    "        np_features_train = np_features_train[:,0,:]\n",
    "        np_features_val = np_features_val[:,0,:]\n",
    "    \n",
    "        input_shape = None\n",
    "    \n",
    "    else:\n",
    "        map_dim = (experiment['radius']*2+1)**2\n",
    "        \n",
    "        np_features_train = np_features_train[:,0:map_dim,:]\n",
    "        np_features_val = np_features_val[:,0:map_dim,:]\n",
    "    \n",
    "        input_shape = (np_features_train.shape[1], np_features_train.shape[2])\n",
    "        \n",
    "    \n",
    "    # compile model\n",
    "    model = get_model(layers_def=experiment['layers'], input_shape=input_shape, learning_rate=experiment['learning_rate'])\n",
    "    \n",
    "    metrics_callback = MetricsCallback(train_data=(np_features_train, np_labels_train),\n",
    "                                       validation_data=(np_features_val, np_labels_val))\n",
    "    \n",
    "    # fit model\n",
    "    model_fit = model.fit(\n",
    "        np_features_train, np_labels_train, \n",
    "        epochs=50, batch_size=experiment['batch_size'],\n",
    "        validation_data=(np_features_val, np_labels_val),\n",
    "        class_weight={0:1.0/label_dist['parallel'], 1:1.0/label_dist['anti-parallel'], 2:1.0/label_dist['no-contact']},\n",
    "        callbacks=[\n",
    "            metrics_callback,\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return model_fit, metrics_callback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if control['run_model'] == False:\n",
    "    print('Skipping model runs...')\n",
    "\n",
    "else:\n",
    "    experiments = get_experiments_file(location=control['experiments'], s3bucket=control['s3bucket'])\n",
    "    \n",
    "    df_results = None\n",
    "    if control['exp_start'] == 'continue':\n",
    "        df_results = load_results(location=control['results'], s3bucket=control['s3bucket'])\n",
    "    \n",
    "    for experiment in experiments:\n",
    "        print('Starting experiment %s' % experiment['id'])\n",
    "        \n",
    "        # check if experiment exists\n",
    "        if df_results is not None and experiment['id'] in df_results['id'].tolist():\n",
    "            print('Skipping experiment since already in df_results...')\n",
    "            continue\n",
    "\n",
    "        model_fit, metrics_callback = run_model(np_features, np_labels, experiment)\n",
    "\n",
    "        # parse results data\n",
    "        experiment['metrics_callback'] = metrics_callback.metrics\n",
    "        experiment['history'] = generalize_history(model_fit.history)\n",
    "        \n",
    "        # add results to dataframe and save csv\n",
    "        df_results = get_summary_results(df=df_results, experiment=experiment)\n",
    "        save_summary_results(df=df_results, location=control['results'], s3bucket=control['s3bucket'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = load_results(location=control['results'], s3bucket=control['s3bucket'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "ax = sns.barplot(x=\"id\", y=\"epoch\", color='b', data=df_results[df_results['class']=='all_macro'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "ax = sns.barplot(x=\"id\", y=\"loss\", hue=\"dataset\", data=df_results[df_results['class']=='all_macro'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(x=\"id\", y=\"accuracy\", \n",
    "                hue=\"dataset\", col=\"class\",\n",
    "                data=df_results[df_results['class'].isin(['all_macro','all_weighted'])], kind=\"bar\",\n",
    "                height=3.5, aspect=2, col_wrap=1)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(x=\"id\", y=\"precision\", \n",
    "                hue=\"dataset\", col=\"class\",\n",
    "                data=df_results, kind=\"bar\",\n",
    "                height=3.5, aspect=2, col_wrap=2)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(x=\"id\", y=\"recall\", \n",
    "                hue=\"dataset\", col=\"class\",\n",
    "                data=df_results, kind=\"bar\",\n",
    "                height=3.5, aspect=2, col_wrap=2)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x=\"recall\", y=\"precision\", \n",
    "                     hue=\"dataset\",\n",
    "                     data=df_results[df_results['class']=='parallel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
